# -*- coding: utf-8 -*-
"""ML_CP.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BCqBXkzluBuiYMOIb6gT2_muo-5JGRmn

# Predicting CLTV with linear regression

<a id='intro'></a>
## Introduction

### CLTV

**Customer Lifetime Value (CLTV)** represents the total amount of money a customer is expected to spend in a business during his/her lifetime. This is an important metric to monitor because it helps to make decisions about how much money to invest in acquiring new customers and retaining existing ones.

### Dataset

For this analysis I am using a public dataset from UCI Machine Learning Repository, which can found [here](http://archive.ics.uci.edu/ml/index.php).

**Attribute information:**

* InvoiceNo: Invoice number. Nominal, a 6-digit integral number uniquely assigned to each transaction. If this code starts with letter 'c', it indicates a cancellation
* StockCode: Product (item) code. Nominal, a 5-digit integral number uniquely assigned to each distinct product.
* Description: Product (item) name. Nominal
* Quantity: The quantities of each product (item) per transaction. Numeric
* InvoiceDate: Invoice Date and time. Numeric, the day and time when each transaction was generated
* UnitPrice: Unit price. Numeric, Product price per unit in sterling
* CustomerID: Customer number. Nominal, a 5-digit integral number uniquely assigned to each customer
* Country: Country name. Nominal, the name of the country where each customer resides.

<a id='dw'></a>
## Data wrangling
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline

from google.colab import drive
drive.mount('/content/drive')

df = pd.read_excel('/content/drive/MyDrive/online_retail_dataset.xlsx', sheet_name='Online Retail')

df.head()

df.info()

"""*Description* and *CustomerID* columns have NULL values"""

df.shape

"""<a id='eda'></a>
## Exploratory data analysis (EDA)

Let us have a look at the data and decide whether we need any data cleaning and data transformation for further analysis.

### NULL values
"""

print('Number of missing values in the dataset:', df.isnull().sum().sum())
print('Percentage of missing values in the dataset:', df.isnull().sum().sum()*100/(df.shape[0]*df.shape[1]))

print("Number of missing values in 'Description' column:", df['Description'].isnull().sum())
print("Percentage of missing values in 'Description' column:", df['Description'].isnull().sum()*100/df.shape[0])

print("Number of missing values in 'CustomerID' column:", df['CustomerID'].isnull().sum())
print("Percentage of missing values in 'CustomerID' column:", df['CustomerID'].isnull().sum()*100/df.shape[0])

"""About 25% of CustomerIDs is missing

### Duplicates
"""

# Check the number of duplicated values
df.duplicated().sum().sum()

# Let's get the duplicated rows
duplicates = df[df.duplicated()]

duplicates[:20]

"""### Number of unique customers"""

df['CustomerID'].nunique()

"""### Number of purchases"""

df['InvoiceNo'].nunique()

"""### Period of time"""

df['InvoiceDate'].min()

df['InvoiceDate'].max()

"""### Country"""

df['Country'].value_counts().plot.bar(color='dodgerblue');

df['Country'].nunique()

"""So we have approximately 1 year data record of 25,900 purchases for 4,372 unique users made in 38 countries (mostly in the UK)

### Quantity and unit price of the products
"""

# Let's look at the price distribution of the products

plt.hist(data=df, x='UnitPrice', color='dodgerblue')
plt.xlabel('Unit price', fontsize=14)
plt.ylabel('Count', fontsize=14)
plt.title("Unit price distribution", fontsize=16);

df['UnitPrice'].describe(percentiles=[.99])

bins = np.arange(0, 21, 2)
ticks = np.arange(0, 21, 2)
plt.hist(data=df, x='UnitPrice', bins=bins, color='dodgerblue')
plt.xticks(ticks, ticks)
plt.xlabel('Unit price', fontsize=14)
plt.ylabel('Count', fontsize=14)
plt.title('Unit price distribution', fontsize=16);

"""The price for most of the products is equal or less than 10£

Let's look at the negative values in *UnitPrice*
"""

df[df['UnitPrice'] < 0]

# Let's look at the quantity of the products

plt.hist(data=df, x='Quantity', color='dodgerblue')
plt.xlabel('Quantity', fontsize=14)
plt.ylabel('Count', fontsize=14)
plt.title("Distribution of quantity of products", fontsize=16);

df['Quantity'].describe(percentiles=[.99])

bins = np.arange(0, 110, 10)
ticks = np.arange(0, 110, 10)
plt.hist(data=df, x='Quantity', bins=bins, color='dodgerblue')
plt.xticks(ticks, ticks)
plt.xlabel('Quantity', fontsize=14)
plt.ylabel('Count', fontsize=14)
plt.title('Distribution of quantity of products', fontsize=16);

"""In most cases customers buy less than 50 items per order.

Let's look at the negative values in *Quantity*
"""

neg_quantity = df[df['Quantity']<0]

neg_quantity.head(10)

neg_quantity.shape

print("The percentage of records with negative Quantity:", neg_quantity.shape[0]*100/df.shape[0])

"""There are 10,624 raws with negative *Quantity* values, which is about 2% out of the total number of records in the dataframe.

<a id='datacleaning'></a>
## Data cleaning and transformation

### Drop NULL values

We have NULL records in columns *CustomerId* and *Description*. Because we are planning to predict CLTV, we need records for each customer ID to proceed calculations. Let's drop rows containing NULL values.
"""

df.dropna(subset=['CustomerID'], inplace=True)

df.shape

# Let's check NULL records in the CustomerId column
df['CustomerID'].isnull().sum()

df.info()

"""### Handle negative quantity"""

df = df.loc[df['Quantity'] > 0]

# Let's check negative records
df[df['Quantity'] < 0]

"""### Remove rows with negative price"""

df = df.loc[df['UnitPrice'] > 0]

# Let's check negative records
df[df['UnitPrice'] < 0]

"""### Handle incomplete data

Period of time for the purchase records is from December 1, 2010 to December  9, 2011. The data for the last month is incomplete. Because we are planning to predict CLTV for the next 3 months and we will be aggregating data monthly, let us ignore the records for the incomplete month.
"""

df = df.loc[df['InvoiceDate'] < '2011-12-01']

# Let's check the time period
df['InvoiceDate'].min(), df['InvoiceDate'].max()

"""### Calculate total sales

Let us create a column for the total sales value for each transaction
"""

df['Sales'] = df['Quantity'] * df['UnitPrice']

df_orders = df.groupby(['CustomerID', 'InvoiceNo']).agg({'Sales': sum, 'InvoiceDate': max})

df_orders.head(8)

df_orders['InvoiceDate'].max(), df_orders['InvoiceDate'].min()

"""<a id='dataanalysis'></a>
## Data analysis

In order to predict and calculate CLTV, we have to estimate the **frequency**, **recency**, and **total amount** of purchases by each customer. We are going to determine basic information about each customer's average and lifetime purchase amount, as well as each customer's duration and frequency of purchase.
"""

def groupby_mean(x):
    return x.mean()

def groupby_count(x):
    return x.count()

def purchase_duration(x):
    return (x.max() - x.min()).days

def avg_frequency(x):
    return (x.max() - x.min()).days / x.count()

groupby_mean.__name__ = 'avg'
groupby_count.__name__ = 'count'
purchase_duration.__name__ = 'purchase_duration'
avg_frequency.__name__ = 'purchase_frequency'

df_summary = df_orders.reset_index().groupby('CustomerID').agg({
            'Sales': [min, max, sum, groupby_mean, groupby_count],
            'InvoiceDate': [min, max, purchase_duration, avg_frequency]
             })

df_summary.head()

"""This data gives us an idea on the purchases each customer made.

Let us have a closer look at the **repeat customers**. Particularly, we are interested in the **number of purchases** and **frequency of purchases** made.

### Repeat customers

#### Number of purchases
"""

df_summary.columns = ['_'.join(col).lower() for col in df_summary.columns]

df_summary

df_summary = df_summary.loc[df_summary['invoicedate_purchase_duration'] > 0]

df_summary.shape

# Let's look at the number of purchases
ax = df_summary.groupby('sales_count').count()['sales_avg'][:20].plot(
        kind='bar',
        color='dodgerblue',
        figsize=(10,8),
        grid=False)
plt.xlabel('Sales count', fontsize=14)
plt.ylabel('Number of customers', fontsize=14)
plt.title('Number of purchases made by repeat customers', fontsize=16);

"""In our analysis, we consider repeat customers who made at least 2 purchases. As you can see, repeat customers tend to make about 12 purchases or less.

#### Average number of days between purchases
"""

ax = df_summary['invoicedate_purchase_frequency'].hist(
        bins=20,
        color='dodgerblue',
        rwidth=0.7,
        figsize=(10,8),
        grid=False)
plt.xlabel('Avearge number of days between purchases', fontsize=14)
plt.ylabel('Number of customers', fontsize=14)
plt.title('Frequency of purchases made by repeat customers', fontsize=16);

"""As you can see from this plot, the majority of repeat customers tend to make a purchase every 12 to 50 days.

### Prepare data
"""

clv_freq = '3M'

df_data = df_orders.reset_index().groupby([
            'CustomerID',
            pd.Grouper(key='InvoiceDate', freq=clv_freq)
            ]).agg({'Sales': [sum, groupby_mean, groupby_count],})

df_data.columns = ['_'.join(col).lower() for col in df_data.columns]

df_data = df_data.reset_index()

df_data.head()

df_data.info()

"""For further simplicity, let us encode *InvoiceDate* column and define in which month a purchase was made"""

map_date_month = {str(x)[:10]: 'M_%s' % (i+1) for i, x in enumerate(
                    sorted(df_data.reset_index()['InvoiceDate'].unique(), reverse=True))}

map_date_month

df_data['M'] = df_data['InvoiceDate'].apply(lambda x: map_date_month[str(x)[:10]])

df_data.head()

# Number of purchases made in each chunk
df_data['M'].value_counts()

"""We marked data with a chunk/time period it belongs to. Now, let us create a set with features and target variables. In order to train a model, we have to transfrom the data into tabular data, where each row represent a costomer and each column represent a feature."""

df_features = pd.pivot_table(
                df_data.loc[df_data['M'] != 'M_1'],
                values=['sales_sum', 'sales_avg', 'sales_count'],
                columns='M',
                index='CustomerID')

df_features.reset_index()

df_features.columns = ['_'.join(col) for col in df_features.columns]

df_features.head()

df_features.reset_index(level=0, inplace=True)

df_features.head()

"""We got Null values after pivoting the table. Let us fill in the Null values with 0."""

df_features.fillna(0, inplace=True)

# Let's check the Null values
df_features.isnull().sum().sum()

df_features.info()

df_features.head()

"""Our target variables will be:"""

df_target = df_data.loc[df_data['M'] == 'M_1', ['CustomerID', 'sales_sum']]

df_target.columns = ['CustomerID', 'CLV_'+clv_freq]

df_target.shape

df_target.head()

df_sample_set = df_features.merge(
                df_target,
                left_on='CustomerID',
                right_on='CustomerID',
                how='left')

df_sample_set.head()

df_sample_set.fillna(0, inplace=True)

df_sample_set.head()

df_sample_set['CLV_3M'].describe()

"""### Build linear regression"""

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression

target_var = 'CLV_3M'
all_features = [x for x in df_sample_set.columns if x not in ['CustomerID', target_var]]

all_features

df_sample_set

X_train, X_test, y_train, y_test = train_test_split(
                                    df_sample_set[all_features],
                                    df_sample_set[target_var],
                                    random_state=2,
                                    test_size=0.35)

lin_reg = LinearRegression(n_jobs=1)

lin_reg.fit(X_train, y_train)

lin_reg.intercept_

coef = pd.DataFrame(list(zip(all_features, lin_reg.coef_)))
coef.columns = ['feature', 'coef']

coef

"""### Evaluate the model"""

from sklearn.metrics import r2_score, median_absolute_error

train_preds =  lin_reg.predict(X_train)
test_preds = lin_reg.predict(X_test)

"""#### R-Squared"""

print('R-Squared for Train set: %0.2f' % r2_score(y_true=y_train, y_pred=train_preds))
print('R-Squared for Test set: %0.2f' % r2_score(y_true=y_test, y_pred=test_preds))

"""R-Squared is the same for the train and test sets. Therefore, we do not have any overfitting or underfitting of the model.

#### Median Absolute Error
"""

print('MedAE for Train set: %0.2f' % median_absolute_error(y_true=y_train, y_pred=train_preds))
print('MedAE for Test set: %0.2f' % median_absolute_error(y_true=y_test, y_pred=test_preds))

"""MedAE has a small difference between test and train sets.

#### Scatter plot
"""

plt.figure(figsize=(8, 6), dpi=80)
plt.scatter(y_train, train_preds, color='dodgerblue')
plt.plot([0, max(y_train)], [0, max(train_preds)], color='gray', lw=1, linestyle='--')

plt.xlabel('Actual Values', fontsize=14)
plt.ylabel('Predicted Values', fontsize=14)
plt.title('Actual vs. Predicted for Train Set', fontsize=16)
plt.grid()

plt.figure(figsize=(8, 6), dpi=80)
plt.scatter(y_test, test_preds, color='dodgerblue')
plt.plot([0, max(y_test)], [0, max(test_preds)], color='gray', lw=1, linestyle='--')

plt.xlabel('Actual Values', fontsize=14)
plt.ylabel('Predicted Values', fontsize=14)
plt.title('Actual vs. Predicted for Test Set', fontsize=16)
plt.grid()

"""As you can see on this plot, the x-axis is the actual values and the y-axis represents the predicted values. Closer the dots are located to the straight diagonal line, better predictions are.

<a id='summary'></a>
## Summary

* Based on the data analysis, we found that the repeat customers tend to make about 12 purchases or less within a year and the majority of repeat customers tend to make a purchase every 12 to 50 days
* We predicted 3-month CLTV for customers of the online retail using linear regression
* R-squared value for the test set is 0.71, which is not great but it is a good benchmark to try other regression models such as Epsilon-Support Vector Regression and Random Forest Regressor
* By knowing CLTV, we can develop positive ROI strategies and make decisions about how much money to invest in acquiring new customers and retaining existing ones.

<a id='reference'></a>
## References

1. Hwang, Y. H. (2019). Hands-on data science for marketing: Improve your marketing strategies with machine learning using Python and R. Birmingham, UK: Packt Publishing.
2. Müller, A. C., &amp; Guido, S. (2018). Introduction to machine learning with Python: A guide for data scientists. Sebastopol, CA: O'Reilly Media.
3. Jeffery, M. (2010). Data-driven marketing the 15 metrics everyone in marketing should know. Hoboken (N.J.), Canada: John Wiley.
4. Customer lifetime Value (CLV) definition - what is customer lifetime Value (CLV). (n.d.). Retrieved February 07, 2021, from https://www.shopify.com/encyclopedia/customer-lifetime-value-clv#:~:text=The%20lifetime%20value%20of%20a,your%20products%2C%20during%20their%20lifetime.
"""

model = lin_reg.fit(X_train, y_train)

# Import streamlit
import streamlit as st

# Display the app title and description
st.title("CLTV Prediction App")
st.write("This app predicts the customer lifetime value (CLTV) of three months based on the past sales data. You can enter the values of the features for a new customer and get the predicted CLTV.")

# Display the input form for a new customer
st.subheader("Predict CLTV for a New Customer")
# Create input fields for each feature
sales_avg_M_2 = st.number_input("Average Sales in Month 2", 0.0, float(df_sample_set['sales_avg_M_2'].max()), 0.0, format="%.2f")
sales_avg_M_3 = st.number_input("Average Sales in Month 3", 0.0, float(df_sample_set['sales_avg_M_3'].max()), 0.0, format="%.2f")
sales_avg_M_4 = st.number_input("Average Sales in Month 4", 0.0, float(df_sample_set['sales_avg_M_4'].max()), 0.0, format="%.2f")
sales_avg_M_5 = st.number_input("Average Sales in Month 5", 0.0, float(df_sample_set['sales_avg_M_5'].max()), 0.0, format="%.2f")
sales_count_M_2 = st.number_input("Sales Count in Month 2", 0, int(df_sample_set['sales_count_M_2'].max()), 0)
sales_count_M_3 = st.number_input("Sales Count in Month 3", 0, int(df_sample_set['sales_count_M_3'].max()), 0)
sales_count_M_4 = st.number_input("Sales Count in Month 4", 0, int(df_sample_set['sales_count_M_4'].max()), 0)
sales_count_M_5 = st.number_input("Sales Count in Month 5", 0, int(df_sample_set['sales_count_M_5'].max()), 0)
sales_sum_M_2 = st.number_input("Sales Sum in Month 2", 0.0, float(df_sample_set['sales_sum_M_2'].max()), 0.0, format="%.2f")
sales_sum_M_3 = st.number_input("Sales Sum in Month 3", 0.0, float(df_sample_set['sales_sum_M_3'].max()), 0.0, format="%.2f")
sales_sum_M_4 = st.number_input("Sales Sum in Month 4", 0.0, float(df_sample_set['sales_sum_M_4'].max()), 0.0, format="%.2f")
sales_sum_M_5 = st.number_input("Sales Sum in Month 5", 0.0, float(df_sample_set['sales_sum_M_5'].max()), 0.0, format="%.2f")

# Create a button to make prediction
if st.button("Predict"):
    # Create a numpy array of the input values
    input_values = np.array([sales_avg_M_2, sales_avg_M_3, sales_avg_M_4, sales_avg_M_5,
                             sales_count_M_2, sales_count_M_3, sales_count_M_4, sales_count_M_5,
                             sales_sum_M_2, sales_sum_M_3, sales_sum_M_4, sales_sum_M_5]).reshape(1,-1)
    # Use the model to make prediction
    prediction = model.predict(input_values)[0]
    # Display the prediction result
    st.success(f"The predicted CLTV for the new customer is {prediction:.2f}")
